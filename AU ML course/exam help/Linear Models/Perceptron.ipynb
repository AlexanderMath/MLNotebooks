{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we implement the Perceptron Learning Algorithm.\n",
    "\n",
    "- Algorithms: Perceptron Learning Algorithm (PLA) and the Pocket extension\n",
    "- DataSets:   \"perceptron\" and \"pocket\"\n",
    "- Video: To Be Added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Perceptron\n",
    "The hypothesis set for the Perceptron is\n",
    "\n",
    "$$ H=\\{h(x)=\\text{sign}(w^T x) \\mid w\\in R^{d+1}\\}$$\n",
    "\n",
    "Given data $D=\\{(x_1, y_1), ..., (x_N, y_N)\\}$ we want to minimize the following error function  \n",
    "\n",
    "$$E_{in}(h)=\\sum_{i=1}^N \\mathbb{1}_{h(x_i)\\neq y_i} $$\n",
    "\n",
    "Here $\\mathbb{1}_{h(x_i)\\neq y_i}$ is the indicator function \n",
    "\n",
    "$$\\mathbb{1}_{h(x_i)\\neq y_i} = \\begin{cases}\n",
    "1 &\\text{if }h(x_i) \\neq y_i\\\\\n",
    "0 &\\text{else}\n",
    "\\end{cases}$$\n",
    "\n",
    "Notice that any hypothesis $h(x)=\\text{sign}(w^Tx)\\in H$ is described exactly by its weights $w\\in R^{d+1}$. Our goal is to find weights $w$ such that we minimize the in-sample error $E_{in}$. \n",
    "\n",
    "Run the following code. It will visualize a set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipython_notebook_importer\n",
    "import DataSet as ds\n",
    "\n",
    "data = ds.DataSet(\"perceptron\", 100)\n",
    "data.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two groups of points could be seperated by a line. We say that the data is *linearly seperable*. The following algorithm is called the Perceptron Learning Algorithm. If the data is *linearly seperable* it will find a seperating line. This will make the perceptron classify all points correctly and thus get $E_{in}(h)=0$ !\n",
    "\n",
    "       Perceptron Learning Algorithm\n",
    "       w = initialize random\n",
    "       while there is a misclassified point x in D\n",
    "          pick misclassified point x\n",
    "          update weights w = w + learning_rate * x * y\n",
    "      \n",
    "It can be proved that \n",
    "\n",
    "$$\\text{D is linearly seperable}\\quad\\Rightarrow \\quad\\text{The PLA algorithm finds a seperating hyperplane in finite steps}$$\n",
    "\n",
    "Several proofs [1,2,3] are available online (the proof is not a part of the AU ML 2017 curriculum). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: Perceptron \n",
    "The following code implements the class `Perceptron`. For simplicity all visualization code has been moved to `hide_visualization_code`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipython_notebook_importer\n",
    "import DataSet as ds\n",
    "import hide_visualization_code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, visualize=False, sleep=0.0):\n",
    "        self.w = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.visualize = visualize\n",
    "        self.sleep = sleep\n",
    "        \n",
    "        if self.visualize: \n",
    "            hide_visualization_code.init_perceptron(self)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Train the Perceptron on data X with labels y. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X:    Matrix with shape (n, d) with data point x_i as the i'th row.   \n",
    "        y:    Array with shape (n, ) with label y_i on the i'th entry. \n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        \n",
    "        # Initialize weights \n",
    "        self.w = np.zeros(d)\n",
    "        \n",
    "        # Get a misclassified point. If all are classified correctly, the \n",
    "        # function will return False. \n",
    "        all_classified_corectly, misclassified_point, misclassified_label = self.misclassified(X, y)\n",
    "\n",
    "        while not all_classified_corectly:\n",
    "            # Visualize if enabled\n",
    "            if self.visualize: self.visualize_step(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.w += self.learning_rate * misclassified_point * misclassified_label\n",
    "        \n",
    "            # Get a new misclassified point. \n",
    "            all_classified_corectly, misclassified_point, misclassified_label = self.misclassified(X, y)\n",
    "            \n",
    "        # Visualize the last round if enabled\n",
    "        if self.visualize: self.visualize_step(X, y)\n",
    "\n",
    "    def misclassified(self, X, y):\n",
    "        \"\"\" Finds a misclassified label, returns False if all points are correctly classified. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X:    Matrix with shape (n, d) with data point x_i as the i'th row.   \n",
    "        y:    Array with shape (n, ) with label y_i on the i'th entry. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        misclassified_point:    The miss classified point, if no such point return False. \n",
    "        misclassified_label:    The label of the miss classified point, if no such point return False. \n",
    "        \"\"\"\n",
    "        # Predict the class of each data point. \n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        # Get the number of miss classified points\n",
    "        misclassified_count = sum(predictions != y)\n",
    "        \n",
    "        # Return False if all points are correctly classified. \n",
    "        if misclassified_count == 0: return True, None, None\n",
    "        \n",
    "        # Filter out the points where predictions disagree with labels. \n",
    "        misclassified_points = X[predictions != y]\n",
    "        misclassified_labels = y[predictions != y]\n",
    "        \n",
    "        # Return te first miss classified point\n",
    "        return False, misclassified_points[0], misclassified_labels[0]\n",
    "        \n",
    "    def error(self, X, y):\n",
    "        \"\"\" Compute the error \"\"\"\n",
    "        n, d = X.shape\n",
    "        return 1/n * sum(self.predict(X) != y) \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predicts the class of X given the trained weights. \"\"\"\n",
    "        return np.sign(X @ self.w)\n",
    "    \n",
    "    def visualize_step(self, X, y, subclass=False): \n",
    "        hide_visualization_code.visualize_perceptron(self, X, y, subclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the `Perceptron` algorithm on the dataset! I have written code that will visualize each step of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.DataSet(\"perceptron\")       \n",
    "\n",
    "perceptron = Perceptron(learning_rate=1, visualize=True, sleep=0.78)\n",
    "perceptron.fit(data.X, data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will eventually find a line seperating the data, however, it might take a while. To visualize the algorithm I added `sleep=0.78` in the code above. This forces the algorithm to pause 0.78 seconds each iteration so we can see what happens. If you want to see the algorithm finish you could try to remove this (it might take between 200-300 iterations to find the line).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pocket\n",
    "Unfortunately, it is very rare that our data is linearly seperable. Try to run the following code, it will visualize data that is not lienarly seperable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.DataSet(\"pocket\")\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the `Perceptron` on this dataset it would run forever. How could we fix this? We change the *while* loop to a *for loop* with T iterations and return the best weights. The best weights are the weight with smallest in-sample error $E_{in}(w)$.   \n",
    "\n",
    "       Pocket Learning Algorithm\n",
    "       w = initialize random\n",
    "       for i=1,...,T\n",
    "          pick misclassified point x (if none stop)\n",
    "          update weights w = w + learning_rate * x * y\n",
    "          compute in-sample error of w \n",
    "          if error is better than previous weights save w \n",
    "      return best w of all T iterations\n",
    "      \n",
    "You can think of this as storing the current best weights in your pocket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code: The Pocket Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the class `Pocket`. It is a simple modification of the class `Perceptron`. It runs $T$ iterations and returns the best hypothesis of the $T$ hypothesis. \n",
    "\n",
    "The `Pocket` class inherits the functions of the `Perceptron` class, so you need to run the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pocket(Perceptron): \n",
    "    \n",
    "    def fit(self, X, y, T):\n",
    "        \"\"\" Train the Perceptron on data X with labels y. At each iteration evaluate the performance\n",
    "        of the current weights. Save the weights with the best performance and return these weights\n",
    "        after T iterations. \"\"\"\n",
    "        n, d = X.shape\n",
    "        \n",
    "        # Initialize weights \n",
    "        self.w = np.zeros(d)\n",
    "        \n",
    "        # Get a misclassified point. If all are classified correctly, the \n",
    "        # function will return False. \n",
    "        all_classified_corectly, misclassified_point, misclassified_label = self.misclassified(X, y)\n",
    "        \n",
    "        # Initialize best error to the worst\n",
    "        best_error = 1.0\n",
    "        self.best_w = np.zeros(d)\n",
    "\n",
    "        for i in range(T): \n",
    "            # Visualize if enabled\n",
    "            if self.visualize: self.visualize_step(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.w += self.learning_rate * misclassified_point * misclassified_label\n",
    "        \n",
    "            # If current error is better than previous update best error and \n",
    "            # best weights. \n",
    "            current_error = self.error(X, y)\n",
    "            if current_error < best_error: \n",
    "                best_error = current_error\n",
    "                self.best_w = np.copy(self.w) # copy and save the best weights.\n",
    "                # If we get zero in-sample error we are done. \n",
    "                if best_error == 0:\n",
    "                    return\n",
    "                \n",
    "            # Get a new misclassified point. \n",
    "            all_classified_corectly, misclassified_point, misclassified_label = self.misclassified(X, y)\n",
    "            \n",
    "        # Visualize the last round if enabled\n",
    "        if self.visualize: self.visualize_step(X, y)\n",
    "            \n",
    "        # Set the final weights to the best weights\n",
    "        self.w = self.best_w\n",
    "            \n",
    "    def visualize_step(self, X, y):\n",
    "        # Let Perceptron draw step as usually, then do our own stuff afterwards. \n",
    "        super().visualize_step(X, y, subclass=True)\n",
    "        hide_visualization_code.visualize_pocket(self, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the Pocket algorithm on some data! The visualization plots the best hypothesis soo far with dashed lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.DataSet(\"pocket\")\n",
    "\n",
    "pocket = Pocket(learning_rate = 1, visualize = True, sleep=0.1)\n",
    "pocket.fit(data.X, data.y, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Breast Cancer\n",
    "So far we tried to run `Perceptron` and `Pocket` on the same dataset, the \"perceptron\" dataset. I generated the dataset myself to allow a nice visualization of the algorithms. In this section we will explore running `Pocket` on real data instead of artificial generated data.\n",
    "\n",
    "The dataset we are going to use is called `breast_cancer` [4]. The input and output of our algorithm will be \n",
    "\n",
    "\\begin{align}\n",
    "X:& \\quad\\text{characteristics of the breast cell nuclei}\\\\\n",
    "y:& \\quad\\text{malevolent or benign (cancer/not cancer)}\n",
    "\\end{align}\n",
    "\n",
    "The original dataset measured 30 different characteristisc of breast cell nuclei. To simplify matters and allow us to visualize our algorithm, I reduced the 30 dimensions to 2 in a \"smart way\" [5]. What I mean by \"smart way\" will be covered later on. For now you should just think of the breast cancer data as being 2d dimensional.  \n",
    "\n",
    "Let's check out the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipython_notebook_importer\n",
    "import DataSet as ds\n",
    "\n",
    "data = ds.DataSet(\"breast_cancer_2d\")\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the `Pocket` algorithm on the dataset for $100$ iterations! You can speed up the time the algorithm takes by disabling visualization (`visualize=False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = ds.DataSet(\"breast_cancer_2d\")\n",
    "\n",
    "iterations = 100\n",
    "p = Pocket(visualize=True, learning_rate=1)\n",
    "p.fit(data.X, data.y, iterations)\n",
    "\n",
    "print(\"Error of best weights: \", p.error(data.X, data.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give around 0.15 in-sample error. You might run try to increase `iterations` and see if this would give you a better in-sample error. It probably wont. The data isn't linearly seperable. Furthermore, many of the blue and green points lie on top of each other. \n",
    "\n",
    "There is one thing we can do to improve performance slightly. Remember that the data originally had more 30 dimensions. Learning directly on the entire data will take more time, but we will probably get better in-sample error! To reduce the time (and because the data isn't 2d) we will turn visualization off (`visualization=False`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.DataSet(\"breast_cancer\")\n",
    "\n",
    "pocket = Pocket(visualize=False)\n",
    "pocket.fit(data.X, data.y, 100)\n",
    "pocket.error(data.X, data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's it. Our first algorithm can predict roughly $90$% correct in-sample on the breast cancer dataset! The next notebooks explore different linear models that will get better in-sample error. That said, there are limitations to linear models; we will later see how introducing non-linearities will significantly improve our error. The breast cancer dataset is also fairly easy, we will later introduce more difficult dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf\n",
    "\n",
    "[2] https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/lec2.pdf\n",
    "\n",
    "[3] http://www.cems.uvm.edu/~rsnapp/teaching/cs295ml/notes/perceptron.pdf\n",
    "\n",
    "[4] http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29\n",
    "\n",
    "[5] I used a technique called Principle Component Analysis (PCA). This will be covered in later iPython Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors, Suggestions and Hall Of Fame\n",
    "If you find any mistakes or have suggestions for improvements reach me at alexmath@cs.au.dk. Any help is very much appreciated, I'll even add your name below for super-awesome everlasting fame!\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (todo) Discussion\n",
    "\n",
    "<iframe src=\"..discussionboard/perceptron.html\" style=\"width: 800px; height: 800px; \"/></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
